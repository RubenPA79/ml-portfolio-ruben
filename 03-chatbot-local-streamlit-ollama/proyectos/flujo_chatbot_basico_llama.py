import requests
import json

class ChatbotBasico:
    def __init__(self, modelo="phi3:mini"):  # Modelo r√°pido por defecto
        self.modelo = modelo
        self.url_ollama = "http://localhost:11434/api/generate"
        self.historial = []
    
    def crear_prompt(self, mensaje_usuario):
        """
        Crea el prompt basado en el template original de Langflow
        """
        prompt_template = """Responde como un asistente experto en Machine Learning EN ESPA√ëOL.

Instrucciones:
- Responde SIEMPRE en espa√±ol
- S√© preciso y t√©cnico cuando sea necesario
- Proporciona ejemplos pr√°cticos
- Si no sabes algo, dilo claramente
- Mant√©n un tono profesional pero amigable

Pregunta del usuario: {input}

Respuesta en espa√±ol:"""
        
        return prompt_template.format(input=mensaje_usuario)
    
    def chat(self, mensaje):
        """
        Env√≠a mensaje a Ollama y obtiene respuesta
        """
        try:
            prompt_completo = self.crear_prompt(mensaje)
            
            payload = {
                "model": self.modelo,
                "prompt": prompt_completo,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_tokens": 500,      # Reducido para ser m√°s r√°pido
                    "num_ctx": 1024,        # Contexto m√°s peque√±o
                    "num_predict": 300      # Menos tokens de predicci√≥n
                }
            }
            
            # TIMEOUT EXTENDIDO A 3 MINUTOS
            response = requests.post(self.url_ollama, json=payload, timeout=180)
            
            if response.status_code == 200:
                resultado = response.json()
                respuesta = resultado.get("response", "Sin respuesta")
                
                # Guardar en historial
                self.historial.append({
                    "usuario": mensaje,
                    "asistente": respuesta
                })
                
                return respuesta
            else:
                return f"Error HTTP {response.status_code}: {response.text}"
                
        except requests.exceptions.Timeout:
            return "‚è±Ô∏è Timeout: El modelo est√° tardando mucho. Intenta con un modelo m√°s r√°pido como 'phi3:mini'."
        except requests.exceptions.ConnectionError:
            return "üîå Error de conexi√≥n: Verifica que Ollama est√© corriendo (`ollama serve`)"
        except Exception as e:
            return f"‚ùå Error inesperado: {str(e)}"
    
    def obtener_historial(self):
        """
        Devuelve el historial de conversaci√≥n
        """
        return self.historial
    
    def limpiar_historial(self):
        """
        Limpia el historial de conversaci√≥n
        """
        self.historial = []
        return "üßπ Historial limpiado"
    
    def cambiar_modelo(self, nuevo_modelo):
        """
        Cambia el modelo LLM
        """
        self.modelo = nuevo_modelo
        return f"üîÑ Modelo cambiado a: {nuevo_modelo}"

# Funci√≥n para testing
if __name__ == "__main__":
    print("ü§ñ Testing ChatbotBasico...")
    
    bot = ChatbotBasico("phi3:mini")  # Modelo r√°pido por defecto
    
    # Test b√°sico
    respuesta = bot.chat("¬øQu√© es el machine learning?")
    print(f"Respuesta: {respuesta}")
    
    # Test historial
    print(f"Historial: {len(bot.obtener_historial())} entradas")